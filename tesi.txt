Abstract
























Introduzione
























1. Dai fondamenti allo stato dell’arte
1.1.1 Definizioni e obiettivi dell’IA
	1.1.2 Il ruolo del Machine learning e del Deep learning
1.2 La rivoluzione dei transformers
	1.2.1 L’architettura transformers
	1.2.2 Il meccanismo di Self-Attention
	1.2.3 Impatto nel NLP e oltre 
1.3 Large Language Models (LLMs) e Foundation Models
	1.3.1 Foundation Models
	1.3.2 Capacità emergenti e Fine-tuning
	1.3.3 Bias e Allucinazioni
1.4 Verso l’Agentic AI

2. AI Agents: Principi, Architetture e il Ruolo dei LLMs
2.1 Introduzione agli AI Agents
2.1.1 Il Ciclo Percezione-Azione e l’ambiente dell’agente
2.1.2 Vantaggi dei sistemi agentici
	2.2 Tassonomia e Tipologie di AI Agents
2.2.1 Agenti a singolo obiettivo vs. Multi-Agente (MAS)
2.2.2 Agenti Reattivi
2.3 Language Agents (LA)
	2.3.1 LLMs come ‘cervello’
2.3.2 Benefici degli Agenti per gli LLMs: Grounding, Interazione, Estendere Limiti di conoscenza
	2.4 Architetture: Dai modelli tradizioni a CoALA
		2.4.1 Architetture tradizionali (DBI o basate su regole)
		2.4.2 Cognitive Architectures for Language Agents (CoALA)
		2.4.3 Componenti chiave in un’architettura CoALA
			2.4.3.1 Modular memory components
			2.4.3.2 Structured Action Space
			2.4.3.3 Generalized Decision-Making Process
		2.4.4 Vantaggi delle architetture cognitive come CoALA
	2.5 Sviluppo e orchestrazione di AI Agents 
2.5.1 Framework per la costruzione di Ai Agents
2.5.1.1 LangChain e LangGraph: Architetture Modulari e Grafo-Basate
2.5.1.2 Modelli Collaborativi e Sistemi Multi-Agente: AutoGen e CrewAI
2.5.2 Altri Framework e Strumenti Emergenti
		
	
		


















Capitolo 1. Dai Fondamenti allo stato dell’arte
	L’intelligenza artificiale (IA) viene definita dal parlamento europeo come “l’abilità di una macchina di mostrare capacità umane quali il ragionamento, l’apprendimento, la pianificazione e la creatività” .
Questa definizione rivela l’ampiezza dell’ambito e le sue implicazioni e racchiude l’essenza di una disciplina che da decenni affascina la ricerca scientifica e, recentemente, l’immaginario collettivo. L’IA non è solo un settore tecnico, ma un ambito che mette in dialogo diverse discipline. Studiare l’IA significa interrogarsi sia su cosa intendiamo per intelligenza, sia su come questa possa essere riprodotta o simulata da sistemi artificiali.
Questa disciplina mira alla costruzione, ma soprattutto alla comprensione, di entità intelligenti. In tal senso, l’aggettivo ‘artificiale’ sottolinea l’origine non biologica mentre il termine ‘intelligenza’ si riferisce all’abilità di questi sistemi di percepire il loro ambiente, di elaborare le informazioni acquisite e di intraprendere azioni strategiche volte a massimizzare le probabilità di successo nel raggiungimento di obiettivi prestabiliti. Non è compito semplice stabilire cosa si intenda per ‘successo’ in questo dominio, ma spesso ciò è quantificabile tramite metriche di performance che ne misurano l’efficacia e l’efficienza in compiti specifici.
L'evoluzione dell'IA moderna si inserisce in una traiettoria storica che riflette l'aspirazione umana di replicare, estendere o potenziare le proprie capacità intellettuali. Dagli automata meccanici del Settecento fino allo sviluppo dei primi calcolatori nel Novecento, la ricerca si è concentrata sulla creazione di sistemi capaci di ragionamento e risoluzione di problemi. Eventi significativi, come la vittoria di Deep Blue di IBM sul campione mondiale di scacchi nel 1997 (Campbell et al., 2002), hanno anticipato l'avvento delle applicazioni contemporanee dell'IA. Tale progressione ha condotto ai complessi sistemi attuali, quali i robot autonomi sviluppati da NVIDIA, sottoposti ad addestramento in ambienti virtuali altamente realistici tramite la piattaforma Omniverse.  In questi contesti simulati, hanno dimostrato capacità avanzate di apprendimento e adattamento, acquisendo autonomamente skill motorie complesse.

Il panorama attuale dell'IA è profondamente influenzato dalla proliferazione dei Big Data, dall'incremento della capacità computazionale e dallo sviluppo di algoritmi sofisticati. Questi fattori hanno collettivamente contribuito all'accelerazione senza precedenti che il campo sta sperimentando.
Questo capitolo si propone di analizzare l'attuale contesto dell'IA, concentrandosi sulle innovazioni che hanno ridefinito i confini della disciplina e che costituiscono la base per lo sviluppo di AI Agents sempre più autonomi e competenti. Saranno esaminate le pietre miliari che hanno plasmato la storia dell'IA, fornendo le fondamenta per comprendere le architetture e i paradigmi caratteristici dell'era attuale.

1.1.1	Definizioni e obiettivi dell’IA
La definizione di Intelligenza Artificiale, un termine coniato per la prima volta da John McCarthy (insieme a Marvin Minsky, Nathaniel Rochester e Claude Shannon) in occasione del convegno di Darmouth nel 1956 (McCarthy et al., 1955), è stata oggetto di dibattito fin dalla nascita di questo campo di studi. Nonostante non esista una risposta universale e unanimemente accettata alla questione "cos'è l'intelligenza" e alle sue manifestazioni, da tali interrogativi sono scaturite discipline fondamentali come le neuroscienze, le quali offrono un vasto bacino di conoscenze per la ricerca in IA.
Una delle definizioni di IA più ampiamente accettate è quella proposta da Russell e Norvig, i quali la inquadrano come lo studio di “agenti che percepiscono il loro ambiente e che intraprendono azioni che massimizzano le loro possibilità di successo” (Russell & Norvig, 2010). Questo approccio 'centrato sull'agente' risulta particolarmente rilevante, poiché enfatizza la capacità di un sistema di operare in autonomia all'interno di un ambiente e di perseguire obiettivi predefiniti.
Questa prospettiva ‘centrata sull’agente’ è particolarmente rilevante poiché pone l’accento sulla capacità del sistema di operare autonomamente in un ambiente e di perseguire obiettivi.
	Le definizioni di Intelligenza Artificiale possono essere categorizzate lungo due dimensioni principali: se il sistema pensa o agisce come un essere umano, e se il sistema pensa razionalmente o agisce razionalmente (Russell & Norvig, 2010). Sebbene la creazione di macchine che "pensano come esseri umani" sollevi complesse questioni filosofiche e cognitive, l'approccio predominante nella ricerca pratica in IA si concentra sulla progettazione di sistemi che "agiscono razionalmente". Questo implica la realizzazione di entità capaci di performare in modo ottimale in un dato contesto, indipendentemente dal fatto che i loro processi cognitivi emulino fedelmente quelli umani.
	Tra gli obiettivi primari dell'IA si annovera la capacità di comprendere e riprodurre comportamenti intelligenti attraverso lo sviluppo di modelli computazionali. Questi modelli devono essere in grado di simulare le capacità umane in domini specifici, quali l’apprendimento e il processo decisionale in condizioni di incertezza. In particolare, è fondamentale che i sistemi siano capaci di apprendere dall'esperienza, adattarsi a nuovi dati e situazioni, e migliorare le proprie prestazioni nel tempo senza la necessità di essere esplicitamente riprogrammati per ogni scenario specifico.
	Pertanto, le ricerche e gli studi in IA del XXI secolo superano la semplice costruzione di macchine capaci di effettuare calcoli ad alta velocità. L'obiettivo è dotarle di una forma di intelligenza funzionale che permetta loro di percepire, ragionare, apprendere e agire in modo efficace e autonomo all'interno di ambienti complessi.

1.1.2 Il ruolo del Machine Learning e del Deep Learning
	L'obiettivo di conferire capacità di apprendimento ai sistemi ha trovato la sua concretizzazione più significativa nei paradigmi del Machine Learning (ML) e, successivamente, del Deep Learning (DL). Il ML si configura come un sottocampo dell'IA che abilita i sistemi ad apprendere dai dati senza un'esplicita programmazione per ogni singolo task (Mitchell, 1997).
	Attraverso l'identificazione di patterns e relazioni in vaste quantità di informazioni, gli algoritmi di ML consentono ai modelli di migliorare iterativamente le proprie prestazioni. Questa capacità di auto-miglioramento ha segnato un distacco fondamentale dai sistemi basati su regole predefinite, una distinzione ulteriormente accentuata dal DL.
Il DL, in quanto evoluzione del ML, si caratterizza per l'impiego di reti neurali artificiali 'profonde', cioè, dotate di numerosi strati nascosti, capaci di apprendere rappresentazioni gerarchiche dei dati. Tale profondità architetturale ha permesso ai modelli di DL, in particolare alle Reti Neurali Ricorrenti (RNNs) e alle Reti Neurali Convoluzionali (CNNs), di estrarre features sempre più astratte e complesse direttamente dai dati grezzi. Questo ha permesso di superare la necessità di un'ingegneria manuale delle caratteristiche (feature engineering), tipica degli approcci ML tradizionali, raggiungendo performance eccezionali in settori quali la visione artificiale e l'elaborazione del linguaggio naturale (NLP).
	Nonostante i loro successi, le architetture tradizionali di DL presentavano limitazioni intrinseche, in particolare nella gestione delle dipendenze a lungo raggio nei dati sequenziali e nella difficoltà di parallelizzazione dell'addestramento. Il superamento di tali limitazioni ha culminato nello sviluppo delle architetture Transformer, le quali hanno ridefinito gli standard di performance nell'IA moderna.

1.2 La rivoluzione dei Transformers
	Se si fosse costretti ad indicare solo alcuni tra i punti cruciali nell’evoluzione dell’IA, l’ideazione dei Transformers sarebbe sicuramente uno di essi. Introdotta nel 2017 da Vaswani et al. nell’articolo “Attention Is All You Need” (Vaswani et al., 2017), l’architettura Transformer ha rappresentato un cambio di paradigma nel modo in cui i modelli di linguaggio elaborano le informazioni. Abbandonando la tradizionale elaborazione sequenziale delle RNNs, i Transformers hanno posto al centro del loro funzionamento un meccanismo innovativo: l’Attention Mechanism. Invece di limitarsi a un’attenzione sequenziale, il modello è in grado di pesare dinamicamente l’importanza di diverse parti dell’input per ogni elemento dell’output, stabilendo connessioni dirette tra parole, anche se lontane, nel testo. Questa capacità di ‘guardare’ l’intero contesto simultaneamente ha permesso di ottenere performance precedentemente inaccessibili e ha aperto la strada a una nuova era, in particolare per algoritmi di NLP più efficienti, scalabili e potenti.














Figura 1. L’architettura Transformer


La rivoluzione dei Transformers non è stata possibile unicamente grazie a innovazioni algoritmiche, ma anche da un parallelo e fondamentale avanzamento tecnologico sul piano dell'hardware, che ha coinciso quasi perfettamente con l'introduzione degli stessi. Tra l’hardware che ha permesso una rapida adozione dei Transformers, rientrano principalmente le Graphics Processing Unit (GPU). Inizialmente concepite per accelerare il rendering grafico nel gaming, le GPU si sono rilevate strumenti computazionali straordinariamente efficienti per le operazioni necessarie nell’addestramento delle reti neurali profonde.  La loro architettura, dotata di migliaia di core di calcolo capaci di operare in parallelo, le ha rese ideali per eseguire le grandissime quantità di calcoli matriciali e vettoriali necessari, come ad esempio il prodotto scalare tra gli innumerevoli word embeddings nel meccanismo di attention, ad una velocità e un’efficienza energetica senza precedenti.
	Tale sinergia ha pavimentato la strada per i Large Language Models (LLMs) e i Foundation Models (FM) che dominano l’attuale campo dell’IA.

1.2.1 L’architettura Transformer
	Originariamente il Transformer si basava su una struttura che può visivamente essere raffigurata come suddivisa in due blocchi encoder-decoder, concepita principalmente per i compiti sequence-to-sequence come la traduzione automatica. Il primo di questi due blocchi, l’encoder, ha il compito di processare la sequenza di input, trasformando le informazioni grezze in una rappresentazione astratta e densa. Il decoder, a sua volta, utilizza questa rappresentazione codificata per generare la sequenza di output desiderata, elemento per elemento.
	Pur essendo questa la forma ‘classica’ del modello, la flessibilità dell’architettura ha favorito la nascita di varianti successive adattate a compiti diversi. Alcuni modelli impiegano soltanto lo stack dell’encoder, privilegiando la capacità di comprensione e rappresentazione del linguaggio. Altri, invece, si basano esclusivamente sul decoder, specializzandosi nella generazione di testo in modalità autoregressiva, ossia producendo un token alla volta in base a quelli precedenti. Questa versatilità ha reso i Transformer la base per un’ampia gamma di sistemi di intelligenza artificiale.
Gli encoder-only models si distinguono per il loro impiego in attività di analisi e interpretazione del linguaggio, in cui l’obiettivo principale è ottenere una rappresentazione contestualizzata dell’input. Un esempio emblematico è BERT (Bidirectional Encoder Representations from Transformers) di Google, che ha ridefinito standard prestazionali in compiti come question answering, classificazione di testi e Named Entity Recognition.
Al contrario, i Decoder-only models trovano applicazione soprattutto nei compiti di generazione, dove il modello produce sequenze linguistiche fluenti e coerenti. Tra gli esempi più noti si trovano i Generative Pre-trained Transformer (GPT) di OpenAI, come GPT-3 e GPT-4 (Brown et al., 2020), nonché i successivi modelli sviluppati da altre realtà, come LLaMA di Meta AI. Questi sistemi hanno mostrato notevoli capacità nella creazione di testi naturali, spaziando dalla scrittura di articoli fino alla produzione automatizzata di codice.
	Per comprendere più a fondo il funzionamento del Transformer è necessario soffermarsi sui passaggi fondamentali svolti all’interno dell’architettura. La prima fase riguarda la trasformazione dei token in vettori numerici multidimensionali, detti embeddings. Questi elementi, che altro non sono che lunghe sequenze di numeri, catturano il significato semantico degli elementi linguistici, ma non forniscono informazioni sulla loro posizione nella sequenza. Per superare questa limitazione, vengono introdotti i positional encoding, vettori che integrano nei dati l’informazione relativa all’ordine delle parole.
	Il nucleo operativo dell’architettura è costituito dal Transformer Block, presente tanto nell’encoder quanto nel decoder. Ogni blocco integra due componenti principali: il meccanismo di Multi-Head Self-Attention, che consente al modello di pesare dinamicamente le relazioni tra i diversi token, anche a distanza, e una rete feed-forward che raffina ulteriormente la rappresentazione. Nel decoder, oltre a questi elementi, compaiono un masked self-attention, che impedisce al modello di accedere a token futuri nella sequenza generata, e uno strato di cross-attention, che stabilisce il collegamento diretto con le informazioni provenienti dall’encoder. Tutti i sottostrati sono accompagnati da connessioni residue e layer normalization, che stabilizzano e migliorano il processo di apprendimento.
	Più blocchi vengono ‘impilati’ in modo consecutivo, il che consente al modello di costruire rappresentazioni sempre più astratte e complesse dei dati.
	Infine, l’output prodotto dall’intera architettura (o soltanto dal decoder, a seconda della configurazione) viene proiettato in uno spazio di probabilità sul vocabolario, solitamente tramite uno strato softmax, da cui viene selezionato il token finale.

1.2.2 Il meccanismo di Self-Attention
	Il cuore dell’innovazione attuata dai Transformer è proprio il meccanismo di Self-Attention. Prima della sua introduzione, le reti neurali sequenziali come le RNNs e le LSTMs (Long Short-Term Memory) faticavano a gestire le dipendenze a lungo raggio nei dati, ovvero la capacità di un modello di correlare informazioni che si trovano distanti all’interno di una sequenza. La loro elaborazione, di natura sequenziale, rendeva complessa la memorizzazione di informazioni rilevanti per periodi prolungati.
	L’intuizione alla base del meccanismo di attenzione è quella di pesare il significato di ogni altra parola nella stessa sequenza al fine di comprenderne appieno il contesto. Si è quindi passati da un’attenzione focalizzata su un input esterno, a un’attenzione interna, in cui il modello attribuisce importanza ai diversi elementi della sua stessa input sequence.
	Il funzionamento del Self-Attention è concettualmente semplice ma molto potente dal punto di vista computazionale: Per ogni token nella sequenza di input, vengono generate tre rappresentazioni vettoriali distinte: la Query (Q), la Key (K) e il Value (V) (Vaswani et al., 2017). Si tratta di tre matrici (o gruppi di vettori) derivate direttamente dall’input embeddings (già arricchiti dal positional encoding) tramite trasformazioni lineari, vale a dire moltiplicazioni per le matrici di peso apprese.
	La Query può essere pensata come una ‘domanda’ che il token corrente pone agli altri token della sequenza. Le Key rappresentano le ‘etichette’ o ‘risposte’ che ogni altro token offre alla domanda posta. I Values sono le ‘informazioni’ o il ‘contenuto effettivo’ che ciascun token trasporta, da pesare in base alle ‘risposte’ delle Key.
	Il calcolo dell’attenzione avviene in diverse fasi. Per prima cosa, vengono calcolati gli attention scores; vale a dire, per ogni token di Query, si calcola un punteggio di compatibilità con tutte le Key presenti nella sequenza. Questo viene tipicamente fatto attraverso il prodotto scalare tra il vettore Query di un token e il vettore Key di ogni altro token (incluso sé stesso). Grazie al prodotto scalare si misura la similitudine tra i vettori, indicando quanto un token sia rilevante per un altro.
	I punteggi ottenuti vengono scalati (dividendo per la radice quadrata della dimensione dei vettori Key, per stabilizzare i gradienti durante l’addestramento) e poi normalizzati utilizzando una funzione softmax. Questa normalizzazione trasforma i punteggi in probabilità, garantendo che la somma dei pesi per ogni Query sia pari a uno. Queste probabilità indicando ‘l’intensità’ dell’attenzione che il token corrente ‘dedica’ a ciascun altro token nella sequenza.
	Infine, ogni vettore Value viene moltiplicato per il proprio peso di attenzione (la probabilità derivata dalla softmax). La somma ponderata di tutti i vettori Value produce il vettore di output per token di Query corrente. Questo vettore di output incorpora le informazioni da tutti gli altri token della sequenza, pesate in base alla loro rilevanza contestuale rispetto alla Query originale.
	Il potere di questo meccanismo si manifesta replicando in parallelo questo intero processo per ogni token nella sequenza, consentendo al Transformer di costruire rappresentazioni contestuali ricche e dinamiche per ciascun elemento, segnando definitivamente un superamento delle limitazioni dei modelli sequenziali.

1.2.3 Impatto nel NLP e oltre
	L’introduzione dei Transformers ha segnato un punto di svolta non solo a livello tecnico, ma anche nelle applicazioni pratiche dell’IA, soprattutto nel campo del linguaggio naturale. Grazie al meccanismo di self-attention, questi modelli hanno reso possibile gestire in modo efficace le dipendenze a lungo raggio e ottenere risultati che i modelli precedenti non raggiungevano.
	Architetture come BERT, che utilizza uno stack di encoder Transformer pre-addestrato su vastissimi corpora testuali, hanno dimostrato capacità eccezionali nella comprensione del linguaggio in una grandissima quantità di contesti. Questo ha rivoluzionato tasks quali il question answering (es. SQuAD dataset ), la classificazione del testo (es. analizzando sentiment in recensioni), e il Named Entity Recognition (NER), dove la capacità di leggere l'intera frase ha permesso una contestualizzazione senza precedenti. I modelli GPT, basati su architetture Transformer di tipo decoder-only, hanno segnato un cambiamento importante nelle applicazioni di generazione del linguaggio. A partire da GPT-3, questi sistemi hanno mostrato una notevole capacità di produrre testi coerenti e fluidi, spaziando dalla scrittura di articoli a contenuti creativi fino alla generazione di codice. L'abilità di questi Large Language Models (LLMs) di svolgere compiti di few-shot learning (apprendere nuovi compiti con pochissimi esempi) ha ulteriormente evidenziato la loro versatilità e la profondità delle rappresentazioni apprese. Numerosi articoli suggeriscono che tali modelli hanno ormai ampiamente superato il famoso test di Turing, inteso come la capacità di ingannare un umano nella percezione della ‘umanità’ delle risposte. Un esempio è uno studio pubblicato su arXiv nel marzo 2025 dove si riporta che GPT-4.5 è stato giudicato umano nel 73% dei casi in un test a tre partecipanti, con maggiore frequenza del vero interlocutore umano (Jones & Bergen, 2025).
	L’influenza del Transformer si è rapidamente estesa oltre i confini del linguaggio: nel campo della computer vision, sebbene le CNNs abbiano dominato per anni, i Vision Transformers (ViT) hanno superato le loro performance nei compiti di classificazione di immagini (Dosovitskiy et al., 2021). I ViT trattano le immagini come una sequenza di patch (piccole parti dell’immagine), applicando il meccanismo di self-attention per catturare le relazioni tra queste patch, similmente a come i Transformers elaborano le parole in una frase. Questo ha aperto nuove frontiere per l’analisi visiva, fornendo un’alternativa potente e spesso più interpretabile alle CNNs.
	Infine, l'architettura Transformer ha giocato un ruolo cruciale anche nello sviluppo di modelli multimodali. Questi sistemi non si limitano più alla sola elaborazione del linguaggio, ma sono in grado di integrare e mettere in relazione informazioni provenienti da diverse modalità, come testo, immagini e persino audio. In altre parole, il Transformer ha aperto la possibilità di superare la tradizionale separazione tra forme di dato eterogenee, consentendo una comprensione più ricca e sfaccettata del mondo.
Un esempio è CLIP (Contrastive Language-Image Pre-Training) (Radford et al., 2021) sviluppato da OpenAI. Il modello sfrutta encoder Transformer distinti per testi e immagini, imparando a cogliere le corrispondenze tra descrizioni linguistiche e contenuti visivi senza la necessità di etichette esplicite. In questo modo, CLIP è in grado di costruire un ponte tra linguaggio e percezione visiva, rendendo possibile associare un concetto testuale a un’immagine o, viceversa, descrivere un contenuto visivo in linguaggio naturale.

1.3 Large Language Models e Foundation Models
	La diffusione capillare dei Transformers è culminata nell’emerge di modelli linguistici su scala senza precedenti, noti come Large Language Models (LLMs). Nonostante il concetto di modello di linguaggio basato su reti neurali fosse già ben consolidato nel campo del NLP, la vera ‘esplosione’ degli LLMs è coincisa con la capacità di sfruttare l’efficienza e, soprattutto, la scalabilità dei Transformers.
Questi modelli, pre-addestrati su dataset testuali vastissimi, contenenti miliardi di token, hanno iniziato a esibire capacità emergenti, vale a dire comportamenti e abilità che non erano presenti in modelli più piccoli o che non erano stati esplicitamente programmati a possedere, come il ragionamento di senso comune o la capacità di apprendere nuovi compiti da pochi o, addirittura, zero esempi. Questo capacità di apprendere da pochi esempi è nota come few-shot learning o zero-shot learning (Wei et al., 2022). 
	Attualmente i LLMs trovano applicazione in moltissimi scenari quotidiani, ad esempio il supporto agli acquisti con Rufus, l’assistente introdotto da Amazon, o la possibilità di porgere domande direttamente nelle chat di WhatsApp, reso possibile dall’integrazione sperimentale dei modelli LLaMA di Meta. Ciò è dovuto non soltanto all’architettura Transformer, che ne costituisce la base strutturale, ma anche alla disponibilità di enormi quantità di dati testuali raccolti dal web: dai libri digitali, alla produzione scientifica fino a giungere alla fonte più vasta e autonomamente prodotta da ogni singolo utente, ovvero i social media. Corpus di ampia portata come Common Crawl, WebText e BooksCorpus hanno offerto la materia prima necessaria per un addestramento in grado di restituire modelli con una conoscenza linguistica e stilistica molto articolata.
	Parallelamente, l’evoluzione e l’uso massiccio di GPU e Tensor Processing Units (TPU), ottimizzate per l’elaborazione parallela, ha reso possibile l’addestramento di modelli con miliardi di parametri in tempi significativamente ridotti rispetto al passato. A questo, si è aggiunta l’innovazione a livello di framework per il deep learning, con TensorFlow e PyTorch, che hanno reso più agevole ed efficiente la gestione distribuita di modelli su centinaia o migliaia di acceleratori.
	Ne deriva la trasformazione degli LLMs in strumenti versatili e concretamente utili, accessibili non soltanto a specialisti del settore ma anche e soprattutto a utenti comuni, seppur con competenze tecnologiche e informatiche minime, segnando un passaggio decisivo verso l’integrazione diffusa dell’intelligenza artificiale nella vita quotidiana. 
Una chiara dimostrazione di questa tendenza è rappresentata dalla progressiva sostituzione dei chatbot, in primo luogo ChatGPT, ai motori di ricerca tradizionali, fenomeno accompagnato da un incremento significativo del loro utilizzo da parte degli utenti e da una parallela riduzione della quota di Google nei contesti di ricerca informativa.

1.3.1 Foundation Models
	Il termine ‘Foundation Model’, coniato dai ricercatori dello Standford Institute for Human-Centered AI (HAI) nel 2001 (Bommasani et al., 2021), identifica una nuova classe di modelli di IA. Nonostante i LLMs come GPT-3 e PaLM (Pathways Language Model, sviluppato da Google AI) siano esempi preminenti e di fatto i primi a ricadere in questa categoria, il concetto di FM è più ampio e non si limita ai soli modelli linguistici. La distinzione fondamentale risiede nel fatto che un LLM è un tipo specifico di FM, uno specializzato nell’elaborazione del linguaggio umano.
	Un FM, invece, può operare anche su diverse modalità di dati (testo, immagini, audio, video) o su combinazioni multimodali, purché sia pre-addestrato su una quantità massiva di dati non etichettati, spesso tramite tecniche di self-supervision.
	Questi modelli, addestrati su tali dati eterogenei e ad ampio spettro, sviluppano una vasta gamma di capacità fondamentali che possono poi essere adattate o ‘specializzate’ (fine-tuned) per una moltitudine di downstream tasks. Concettualmente, un FM è progettato per servire da ‘base’ per un’ampia varietà di applicazioni, trasferendo la conoscenza acquisita a compiti più specifici.

1.3.2 Capacità emergenti e Fine-Tuning
	Tradizionalmente, lo sviluppo di sistemi di IA richiedeva dataset etichettati specifici e spesso costosi da etichettare, sia economicamente che temporalmente, per ogni nuovo compito. Come detto, I FM hanno rivoluzionato questo paradigma. Grazie al loro vasto pre-addestramento su dati non etichettati, sono in grado di apprendere rappresentazioni dense e ricche della conoscenza e del linguaggio. È proprio questo che consente di adattarli a nuovi tasks riducendo drasticamente i costi e i tempi di sviluppo.
	La scala dei FM, sia in termini di parametri che di dati di addestramento, porta all’emergere di capacità inattese, come la risoluzione di problemi matematici o la comprensioni di istruzioni complesse, che non erano esplicitamente programmate o presenti in modelli più piccoli (Wei et al., 2022).
	La creazione di questi sistemi è fondamentale poiché fungono da piattaforme unificate per lo sviluppo di svariate applicazioni di IA. Invece di sviluppare un modello da zero per ogni problema, gli sviluppatori possono così partire da un FM consolidato, risparmiando risorse computazionali e umane. Questo promuove una maggiore standardizzazione delle metodologie e consente una rapida iterazione e implementazione di nuove soluzioni.
	La disponibilità di modelli con capacità così ampie accelera la ricerca e permette ai ricercatori di concentrarsi sull’adattamento e l’applicazione di queste basi piuttosto che sulla costruzione dell’architettura da zero. Ciò significa che l’innovazione può avvenire a un ritmo più elevato, con un maggiore focus sulla risoluzione di problemi del mondo reale.

1.3.3 Bias e allucinazioni
	Nonostante i loro innegabili vantaggi, i FM presentano anche sfide significative, tra cui i requisiti computazionali estremamente elevati per l’addestramento, il potenziale di propagare bias presenti nei dataset di pre-addestramento, e la tendenza a generare ‘allucinazioni’. Queste criticità non solo compromettono l’affidabilità e l’equità dei modelli, ma sollevano anche importanti questioni etiche e sociali.
	I bias presenti nei Foundation Models derivano direttamente dal processo di addestramento su grandi quantità di dati preesistenti. I testi e i contenuti multimodali utilizzati riflettono infatti disuguaglianze, stereotipi e limiti tipici della società e delle fonti digitali. Di conseguenza, il modello non assimila soltanto conoscenze linguistiche e fattuali, ma anche pregiudizi impliciti, come quelli legati al genere, all’etnia o ad aspetti culturali. Questo può portarlo, anche in modo non intenzionale, a riprodurre o persino a rafforzare tali distorsioni nei propri output.
	Ad esempio, un modello potrebbe associare determinate professioni a un genere specifico o mostrare preferenze ingiustificate verso certi gruppi etnici, riflettendo le correlazioni statistiche presenti nel training dataset piuttosto che una comprensione equa della realtà. Questo comporta gravi implicazioni, soprattutto quando i FMs vengono impiegati in contesti critici come la selezione del personale, la diagnosi medica o la profilazione del rischio, dove decisioni basate su output-biased possono avere conseguenze socio-economiche deleterie.
	Parallelamente, la capacità dei LLMs in generale è spesso accompagnata da un’altra problematica nota come allucinazione (hallucination). Questo termine si riferisce alla tendenza del modello a generare informazioni che appaiono plausibili e coerenti, ma che sono di fatto errate, prive di fondamento fattuale o persino contradditorie rispetto alla realtà. Le allucinazioni possono manifestarsi in varie forme: dalla generazione di citazioni bibliografiche inesistenti, a fatti storici errati, a descrizioni di eventi mai accaduti. La natura stocastica e probabilistica del processo di generazione, unita alla vastità della conoscenza interna al modello e talvolta alla mancanza di un meccanismo di grounding  esterno verificabile, rende i LLMs suscettibili a questi errori.
	La mitigazione di questi bias e allucinazioni è un’area di ricerca attiva: richiede approcci che spaziano dall’analisi e pre-processing dei dati di addestramento per ridurre i bias intrinseci, allo sviluppo di metodi di fine-tuning e prompt engineering che incentivino risposte più imparziali e accurate, fino all’implementazione di sistemi di verifica esterna (fact-checking) che possano groundare le risposte del modello su fonti autorevoli.

1.4 Verso l’Agentic AI
	L’evoluzione dell’Intelligenza Artificiale sta convergendo verso un nuovo e promettente paradigma: quello dell’Agentic AI. Se i modelli di linguaggio tradizionali eccellono nell’elaborazione nella generazione di testo statico, lo stato dell’arte attuale in campo AI evidenza una crescente tendenza verso lo sviluppo e il proliferare di sistemi che vanno oltre la mera previsione o produzione di output basato su un singolo prompt. Questi sistemi, definiti agenti, sono progettati per operare in modo più autonomo e dinamico, simulando facoltà cognitive e comportamenti intelligenti in contesti complessi.
	Il termine ‘agente’ deriva dal latino agens, participio presente di agere, che significa ‘fare’, ‘agire’. Nel contesto dell’Intelligenza Artificiale, un agente è un’entità che è in grado di percepire il proprio ambiente tramite attuatori (Russell & Norvig, 2010). La loro essenza risiede nella capacità di essere proattivi e reattivi, di perseguire obiettivi specifici e di adattare il proprio comportamento in funzione delle osservazioni che ne derivano. È evidente quindi la volontà di distinguerli dai semplici programmi o modelli passivi. Un agente non si limita a rispondere a un input immediato; è bensì un sistema che esegue un ‘ciclo di decisione’ persistente, che può includere pianificazione, ragionamento, accesso a memorie esterne e interazione continua con l’ambiente (Wang et al., 2024). L’obiettivo primario di questa integrazione è estendere le capacità intrinseche degli LLMs, che pur dotati di vasta conoscenza e abilità linguistiche, sono di per sé ‘statici’ e privi di un meccanismo di auto-riflessione o interazione col mondo esterno.
	Nel capitolo successivo verrà analizzato in profondità il tema degli AI Agents, con particolare attenzione alle architetture che ne costituiscono la base. Si prenderanno in esame le diverse tipologie di approcci adottati per la loro progettazione, mettendo in evidenza i vantaggi che offrono in termini di flessibilità e capacità di problem solving. Parallelamente, saranno discussi anche i limiti e i rischi intrinseci a tali architetture, che ne influenzano l’affidabilità e l’impiego in contesti reali. Infine, l’analisi si concentrerà sugli strumenti e framework contemporanei, come LangChain e LangGraph, che facilitano la costruzione e l’orchestrazione di agenti complessi, insieme a metodologie emergenti che puntano a rendere lo sviluppo di sistemi di Agentic AI sempre più scalabili ed efficienti.



2. AI Agents: Principi, Architetture e il Ruolo dei LLMs
	Il precedente capitolo ha esplorato l’evoluzione dell’Intelligenza Artificiale, culminando nella rivoluzione dei Transformers e nell’ascesa dei LLMs e FM. Si è evidenziato come l’innovazione architetturale, unita alla disponibilità di big data e alla crescente potenza computazionale, abbia permesso a questi modelli di raggiungere livelli di comprensione e generazione del linguaggio senza precedenti, manifestando persino capacità emergenti. 
	Tuttavia, per quanto sofisticati, gli LLMs da soli rimangono in larga parte “statici”, eccellendo nella produzione di output basato su un singolo prompt ma privi di una capacità intrinseca di interagire autonomamente con ambienti dinamici, di sostenere una sequenza di decisione nel tempo o di gestire memorie complesse.
	È in questo contesto che emerge il paradigma degli AI Agents: sistemi progettati per estendere le capacità degli LLMs, dotandoli di meccanismi di percezione, ragionamento, pianificazione e azione, consentendo loro di operare in modo più autonomo e proattivo.

2.1 Introduzione agli AI Agents
	Un agente intelligente può essere definito come qualsiasi entità capace di percepire il proprio ambiente attraverso sensori e di agire su tale ambiente tramite attuatori (Russel and Norvig, 2013). Questa definizione, considerata la più ampia, delinea l’agente come un sistema autonomo che opera in un contesto, cercando di raggiungere specifici obiettivi attraverso un’interazione dinamica e continua.
	La spinta verso lo sviluppo di sistemi agentici è stata storicamente duplice. Da un lato, l’AI ha ambito a simulare e poi a superare le capacità cognitive umane, necessitando di entità che non si limitassero a elaborare informazioni in modo passivo, ma che potessero intervenire attivamente. Dall’altro lato, la crescente complessità dei compiti e degli ambienti operativi ha richiesto soluzioni che potessero adattarsi e funzionare in condizioni incerte o mutevoli, senza che l’agente fosse stato esplicitamente istruito per ogni possibile scenario. L’integrazione dei progressi nei LLMs ha ulteriormente accelerato questo campo, permettendo ai LA di utilizzare gli LLMs per interagire con il mondo.
	Gli AI Agents sono dunque sistemi che incorporano gli avanzamenti più recenti negli LLMs al consolidato campo storico della progettazione di agenti (Russel and Norvig, 2013). Questa sintesi permette vantaggi reciproci. Se da un lato gli LLMs, pur semanticamente potenti, posseggono capacità di conoscenza e ragionamento intrinsecamente limitate, i LA mitigano queste problematiche connettendo gli LLMs a meccanismi di memoria interna e a sistemi di interazione con ambienti esterni, fornendo loro un ‘grounding’ nella conoscenza esistente o nelle osservazioni dirette (Wang et al., 2024).
	D’altro canto, gli agenti tradizionali spesso richiedevano la programmazione di regole handcrafted o si basavano esclusivamente su tecniche di reinforcement learning (Sutton and Barto, 2018), rendendo la loro generalizzazione a nuovi ambienti e compiti una sfida complessa. I LA, invece, attingono ai commonsense priors presenti negli LLMs per adattarsi a nuove attività, riducendo la dipendenza dall’annotazione umana o dall’apprendimento per tentativi ed errori (Wang et al., 2024).
	Mentre gli agenti più ‘datati’ utilizzavano gli LLMs prevalentemente per selezionare o generare azioni semplici, le generazioni più recenti li impiegano per compiti più complessi quali il ragionamento (Yao et al., 2022b), la pianificazione (Hao et al., 2023; Yao et al., 2023) e la gestione della memoria a lungo termine, al fine di migliorare i processi decisionali. Questa ultima generazione di agenti cognitivi basati sul linguaggio utilizza processi interni notevolmente sofisticati (Wang et al., 2024).

2.1.1 Il Ciclo Percezione-Azione e l’Ambiente dell’Agente
	Il comportamento di qualsiasi agente intelligente è governato da un ciclo percezione-azione continuo e iterativo (Russel and Norvig, 2013). Questo ciclo descrive la sequenza fondamentale di operazioni attraverso cui un agente interagisce con il mondo esterno. Esso inizia con la percezione, fase in cui l’agente acquisisce informazioni dal suo ambiente tramite sensori. Tali percezioni possono variare dall’input testuale per un LA alla lettura di sensori fisici per un robot, fino all’osservazione di stati di un simulatore (Sutton and Barto, 2018). Le informazioni percepite vengono poi elaborate per aggiornare lo stato dell’agente, ragionare sul contesto e prendere una decisione appropriata. 
Questa decisione culmina nella fase di azione, in cui l’agente esegue operazioni che modificano l’ambiente stesso, o il proprio stato interno, tramite attuatori (Russel and Norvig, 2013). Il risultato di tale azione genera nuove percezioni, riavviando il ciclo in una dinamica continua che permette all’agente di adattarsi e perseguire i propri obiettivi.
L’efficacia e la complessità di un agente dipendono fortemente dalla natura dell’ambiente in cui opera. L’ambiente può essere descritto attraverso diverse proprietà fondamentali che ne influenzano il funzionamento. Un primo aspetto riguarda il grado di osservabilità: in alcuni casi l’agente ha accesso completo allo stato dell’ambiente, che viene definito quindi pienamente osservabile, mentre in altri può disporre soltanto di una conoscenza parziale. In quest’ultima circostanza diventa necessario che l’agente mantenga uno stato interno o un modello del mondo capace di sopperire alle informazioni mancanti.
Un’ulteriore distinzione riguarda il determinismo. Un ambiente viene definito deterministico quando lo stato successivo è interamente stabilito dallo stato corrente e dall’azione compiuta dall’agente, mentre si definisce stocastico quando l’evoluzione dipende anche da fattori esterni non controllabili. Un’altra caratteristica rilevante è l’episodicità: negli ambienti episodici le decisioni dell’agente si configurano come esperienze indipendenti tra loro, mentre negli ambienti sequenziali le azioni presenti influenzano le percezioni e le scelte future, richiedendo dunque strategie di pianificazione a lungo termine.
Va inoltre considerato il grado di staticità. In un ambiente statico i cambiamenti dipendono esclusivamente dalle azioni dell’agente, mentre in un ambiente dinamico il cambiamento di stato può avvenire anche in modo autonomo, indipendentemente dalle attività dell’agente. 
Infine, un’ultima distinzione riguarda la natura discreta e continua dell’ambiente, legata sia al numero degli stati possibili sia alla tipologia di percezioni e di azioni disponibili.
La progettazione di un agente intelligente deve necessariamente tener conto di queste caratteristiche, poiché esse influenzano in maniera diretta sulla complessità del ciclo percezione-azione e sui meccanismi interni richiesti per garantire un funzionamento efficace (Russell e Norvig, 2013). Nel caso specifico dei LA (Language Agents), l’ambiente si configura tipicamente come un contesto discorsivo, oppure come l’insieme di strumenti esterni e di API che consentono l’interazione con il mondo digitale (Wang et al., 2024).

2.1.2 Vantaggi dei Sistemi Agentici
	L’adozione di un approccio basato sugli AI Agents offre numerosi vantaggi rispetto a sistemi di IA tradizionali in particolari contesti come in applicazioni complesse e caratterizzate dal dinamismo.
	I sistemi agentici eccellono nella capacità di adattarsi ad ambienti mutevoli e situazioni impreviste. A differenza dei sistemi basati su regole fisse, un agente può modificare il proprio comportamento in base alle nuove percezioni, apprendendo dall’esperienza e adeguando le proprie strategie. Questa flessibilità è cruciale in contesti reali dove lo scenario operativo è raramente statico. 	
	Alla base del principio stesso degli agenti, c’è la progettazione per operare con un certo grado di autonomia, perseguendo obiettivi senza la necessità di un intervento umano continuo. Questa proattività consente loro di avviare azioni in risposta a cambiamenti nell’ambiente o al progredire di un compito, riducendo il carico cognitivo sugli operatori umani e abilitando l’automazione di processi complessi.
	Tra i vantaggi più significativi si annovera la capacità di scomporre problemi complessi in sotto-problemi più gestibili, assegnando la risoluzione a moduli o sub-agents specializzati. Nel caso dei LA, questa scomposizione può tradursi nell’utilizzo di LLMs per il ragionamento e la pianificazione di alto livello, delegando l’esecuzione di azioni specifiche a strumenti esterni (Wang et al., 2024).  

2.2 Tassonomia e Tipologie di AI Agents
	La presenza di diverse classificazioni e tipologie di AI Agents riflette la varietà di problemi che tali sistemi sono chiamati a risolvere e la diversità delle architetture sottostanti. Una tassonomia chiara è fondamentale per comprendere le capacità, e limitazioni e gli ambiti di applicazione di ciascuna classe di agenti. Questa categorizzazione è spesso basata sul grado di ‘intelligenza’, autonomia, sul modo in cui percepiscono e agiscono nell’ambiente, e sulla loro capacità di apprendimento. Tale distinzione aiuta a posizionare gli agenti lungo uno spettro di complessità, che va dai comportamenti semplici e reattivi a quelli sofisticati e proattivi.

2.2.1 Agenti a Singolo Obiettivo vs. Multi-Agente 
	Una delle distinzioni fondamentali nella categorizzazione degli AI Agents riguarda il numero di entità coinvolte nel processo decisionale e nella risoluzione di un problema. Si distinguono, quindi, gli agenti a singolo obiettivo dai sistemi multi-agente (MAS - Multi-Agent Systems).
	Gli agenti a singolo obiettivo operano in maniera isolata o, comunque, senza una diretta e complessa interazione o coordinamento con altri agenti per il raggiungimento del proprio scopo. Il loro focus è interamente rivolto all’ottimizzazione delle proprie azioni e del proprio stato interno per massimizzare la performance individuale in un dato task. Questi agenti possono essere altamente sofisticati e capaci nella loro intelligenza interna e capacità di apprendimento, ma la loro concezione non include meccanismi espliciti per la comunicazione, la negoziazione o la collaborazione con altre entità autonome. Un singolo LA progettato per generare testo o rispondere a query basate su un proprio prompt, senza quindi dover interagire con altri agenti per raggiungere il suo fine ultimo, è un esempio di questa tipologia di sistema.
	Al contrario, i sistemi multi-agente sono composti da più agenti individuali che operano in un ambiente condiviso e interagiscono tra loro per perseguire un obiettivo comune o per raggiungere obiettivi individuali che, complessivamente, contribuiscono a un risultato sistemico. La complessità dei MAS deriva proprio dalla necessità di gestire il coordinamento, la comunicazione, la cooperazione e, talvolta, la competizione tra gli agenti (Wooldridge, 2009). In questi sistemi, emerge inoltre una forma di intelligenza collettiva, dove l’aggregazione delle capacità e delle interazioni degli agenti supera la somma delle singole parti. I MAS trovano applicazione in scenari dove la natura del problema è distribuita o richiede diverse forme di specializzazione e cooperazione: “in alcuni scenari, la gestione di un compito potrebbe essere molto più complessa se affidata ad un singolo agente” (Russel and Norvig, 2013). Un esempio emblematico in adozione su larga scala di MAS in ambiti come la logistica autonoma è Amazon, che impiega migliaia di robot autonomi nei suoi magazzini per gestire lo smistamento, il trasporto e lo stoccaggio delle merci. Pur operando individualmente, questi robot comunicano e coordinano le loro azioni attraverso un sistema centrale di gestione per ottimizzare l’intero processo logistico. 
	Lo sviluppo di MAS richiede una particolare attenzione ai protocolli di comunicazione e ai meccanismi di risoluzione dei conflitti, elementi che sono diventati particolarmente rilevanti con l’integrazione di LA dotati di LLMs, capaci di interazioni linguistiche complesse anche tra agenti stessi.
	
2.2.2 Agenti Reattivi
	All’estremità più semplice dello spettro dei sistemi agentici, si collocano gli Agenti Reattivi. Questi agenti si distinguono per la loro capacità di rispondere in modo diretto e immutabile alle situazioni attuali percepite dall’ambiente, senza mantenere alcuna memoria di eventi passati o senza la capacità di pianificare azioni future (Russel and Norvig, 2013). La loro logica decisionale è tipicamente basata su una semplice mappatura condizione-azione del tipo: “se questa condizione è vera, allora esegui questa azione”. Non possiedono un modello interno del mondo né capacità di ragionamento esplicito.
	La semplicità intrinseca degli Agenti Reattivi conferisce loro diversi vantaggi, tra cui l'efficienza computazionale e la rapidità di risposta, poiché non richiedono complesse elaborazioni interne. Tuttavia, tale semplicità rappresenta anche la loro maggiore limitazione. Essi sono incapaci di apprendere, di adattarsi a situazioni non previste dal loro set di regole iniziali, o di perseguire obiettivi a lungo termine. La loro "intelligenza" è superficiale, determinata esclusivamente dalle dirette correlazioni tra percezioni e azioni predefinite (Wooldridge, 2009). Nonostante queste limitazioni, gli Agenti Reattivi hanno trovato e continuano a trovare utilità in domini specifici dove l'ambiente è relativamente semplice e prevedibile, o dove la velocità di risposta è un requisito primario e il comportamento complesso può emergere dall'interazione di molteplici agenti semplici. 

2.3 Language Agents
	L'avvento di modelli linguistici di grandi dimensioni ha segnato una convergenza fondamentale tra l'ambito dell'Intelligenza Artificiale e quello dell'Elaborazione del Linguaggio Naturale. In questo scenario, una classe di sistemi di IA ha guadagnato crescente importanza: i Language Agents. È possibile considerare questi agenti come una sintesi innovativa che applica i più recenti progressi negli LLMs al campo della agent design (Russell and Norvig, 2013). La peculiarità di questi sistemi risiede nella loro capacità di utilizzare il linguaggio naturale non solo come mezzo di comunicazione, ma come strumento centrale per il ragionamento, la pianificazione e l’interazione con ambienti complessi.
	A differenza degli agenti reattivi, discussi in precedenza, che operano su una mappatura diretta condizioni-azioni, o degli stessi LLMs ‘stand-alone’ che si limitano a generare risposte a prompt immediati, i LA sono concepiti per un ‘ciclo di decisione’ persistente e sofisticato. Questo ciclo abilita una profonda interazione con il mondo, dove il linguaggio funge da interfaccia universale per comprendere le percezioni, formulare intenzioni e eseguire azioni in modo autonomo (Wang et al., 2024). L’obiettivo primario di questa categoria di agenti è colmare il divario tra le robuste capacità di comprensione e generazione linguistica dei LLMs e la necessità di sistemi capaci di interagire dinamicamente, gestire memoria a lungo termine e pianificare strategicamente sia in contesti reali che simulati.

2.3.1 LLMs come ‘cervello’
	Si potrebbe dire che, nelle architetture agentiche attuali, i LLMs assumono un ruolo centrale arrivando a fungere da ‘cervello’ del sistema. Questa metafora è fondamentale per sottolineare la loro funzione primaria di processore cognitivo, in grado di orchestrare il comportamento dell’agente. L’LLM, infatti, è responsabile di interpretare le percezioni dall’ambiente, di generare risposte coerenti e di formulare piani d’azione, anche complessi. Tale funzionalità si estende a compiti come il ragionamento e la gestione della memoria, sebbene gli LLMs da soli posseggano capacità di conoscenza e ragionamento limitate, necessitando di essere connessi a memoria interna e ambienti esterni per mitigare tali problematiche (Wang et al., 2024). In sistemi più semplici, l'LLM può operare come un interprete diretto del contesto, determinando l'azione successiva; in architetture più avanzate, come quelle cognitive, l'LLM funge da motore inferenziale, guidando processi decisionali sofisticati.

2.4 Architetture: Dai modelli tradizionali a CoALA
	La complessità e l’efficacia di un AI Agent dipende fortemente dalla sua architettura interna, ovvero al modo in cui i suoi vari componenti sono organizzati e interagiscono per implementare il ciclo percezione-azione (Russel and Norvig, 2013). Storicamente, la progettazione di architetture agenti ha seguito diverse filosofie, sviluppando paradigmi che riflettevano le capacità tecnologiche e le esigenze computazionali del tempo. Dalle semplici strutture reattive fino a sistemi più complessi dotati di capacità di pianificazione e ragionamento, ogni architettura ha cercato di bilanciare tre necessità: efficienza, flessibilità e robustezza. Questa sezione esplorerà alcune delle architetture agenti tradizionali, per poi approfondire la proposta recente di un framework basato su architetture cognitive per LA, come CoALA (Cognitive Architectures for Language Agents), che mira a strutturare in modo esplicito processi come l'apprendimento, la memoria e la decisione (Wang et al., 2024).

2.4.1 Architetture tradizionali
	Sebbene l’avvento dei LLMs abbia aperto nuove prospettive di ricerca e, soprattutto, applicazione, la comprensione di approcci storici alla creazione di architetture agentiche rimane centrale per cogliere l’evoluzione del campo e le motivazioni che hanno condotto verso architetture più sofisticate.
	Un primo paradigma è rappresentato dai Rule-Based Agents, i quali operano attraverso un insieme predefinito di regole condizionali sotto forma di ‘if-then’. In ambienti ben strutturati, tali agenti si dimostrano particolarmente efficienti e trasparenti, poiché il loro comportamento è direttamente interpretabile. Tuttavia, la loro capacità di scalare verso domini più complessi si rivela limitata: la crescita esponenziale delle condizioni rende la base di regole fragile e difficilmente gestibile, richiedendo un’intenza attività di programmazione manuale. Inoltre, la mancanza di capacità di apprendimento rende questo approccio inadeguato in scenari dinamici o non completamente conosciuti.
	Per superare tali vincoli si sono sviluppati i Model-Based Agents, caratterizzati dal mantenimento di una rappresentazione interna del mondo circostante. Questo modello consente di inferire stati non direttamente osservabili e di comprendere le conseguenze delle azioni sia sull’ambiente sia sulla sua evoluzione. Tale approccio si rivela particolarmente rilevante in contesti di navigazione robotica o in ambienti parzialmente osservabili, poiché garantisce maggiore autonomia e adattabilità rispetto agli agenti puramente reattivi. D’altro canto, la costruzione e la manutenzione di un modello accurato si rivelano onerose dal punto di vista computazionale e richiedono una costante ‘calibrazione’.
	Un ulteriore passo avanti è costituito dai Goal-Based Agents, che integrano un modello del mondo con una chiara definizione degli obiettivi da raggiungere. Essi utilizzano la conoscenza interna e i fini prefissati per pianificare sequenze di azioni mirate a condurre lo stato interno del sistema verso quello desiderato. Tale architettura introduce un comportamento che potrebbe definirsi proattivo e orientato al futuro, rivelandosi cruciale nei compiti di pianificazione automatica. La complessità della pianificazione rappresenta tuttavia un limite significativo: in ambienti ad alta ramificazione di stati e azioni, il processo può diventare intrattabile.
	A generalizzare questo approccio si collocano gli Utility-Based Agents, che, oltre a perseguire obiettivi specifici, mirano alla massimizzazione di una funzione di utilità (Russell and Norvig, 2013). Quest’ultima esprime un grado di preferenza rispetto ai risultati ottenuti, incorporando dimensioni quali costo, tempo o rischio. Ciò consente di valutare non solo il raggiungimento dell’obiettivo, ma anche la qualità con cui esso è stato conseguito. L’architettura Utility-Based si rivela pertanto cruciale in scenari decisionali caratterizzati da incertezza e compromessi, permettendo di selezionare strategie ottimali. Rimane, tuttavia, la difficoltà nella definizione e nell’ottimizzazione della funzione di utilità stessa, che rappresenta non soltanto una sfida applicativa, ma anche teorica.

2.4.2 Cognitive Architectures for Language Agents (CoALA) 
	Uno degli approcci più avanzati, capaci di integrare le capacità cognitive degli LLMs con meccanismi strutturati di gestione della memoria e del ragionamento, è rappresentato dalla proposta di Cognitive Architectures for Language Agents (CoALA), un framework concettuale progettato per descrivere e costruire agenti linguistici avanzati.
	CoALA trae ispirazione dalla ricca storia della scienza cognitiva e dell’intelligenza artificiale simbolica, connettendo intuizioni decennali con la ricerca di frontiera sui grandi modelli linguistici (Wang et al., 2024).
	Le architetture CoALA sono pensate per dotare i LA di una struttura interna più sofisticata, che consenta loro di effettuare ragionamenti complessi, pianificare azioni in modo efficace e gestire un’ampia gamma di informazioni tramite meccanismi di memoria modulari. A differenza di approcci più semplici che integrano l’LLM in un loop di feedback diretto con l’ambiente (Figura 2), le architetture cognitive per i LA utilizzano l’LLM per gestire uno stato interno più evoluto dell’agente attraverso processi di apprendimento e ragionamento (Wang et al., 2024).

 
Figura 2. Diversi usi di un LLM

Questo permette agli agenti non solo di processare input e generare output, ma anche di mantenere una rappresentazione coerente e aggiornata del loro ambiente e di sé stessi, fondamentale per compiti che richiedono persistenza, adattamento e capacità di apprendimento profondo. L’LLM, in questo contesto, agisce come il ‘cervello’ centrale dell’agente, ma è affiancato da componenti aggiuntive che ne estendono le capacità di gestione della memoria e di interazione strategica con il mondo.
	La proposta di CoALA mira a fornire un modello e una tassonomia per organizzare sia gli agenti esistenti sia per guidare lo sviluppo futuro. Questo framework si pone come un blue-print per strutturare tali agenti, fornendo una lente concettuale attraverso cui analizzare e costruire LA con capacità cognitive più evolute.

2.4.3 Componenti chiave in un’architettura CoALA
	Le architetture CoALA si distinguono per la loro concezione modulare e per l’integrazione esplicita di capacità simili a quelle cognitive umane, gestite e orchestrare da LLMs. Questa strutturazione permette agli agenti di superare le limitazioni di memoria e ragionamento dei LLMs stand-alone, dotandoli di una capacità di interagire con il mondo e di evolvere nel tempo. Secondo il framework CoALA, un Cognitive Language Agent è caratterizzato da tre componenti chiave interconnesse: i Modular Memory Components, uno Structured Action Space e un Generalized Decision-Making Process (Wang et al., 2024). Queste componenti lavorano in sinergia per permettere all’agente di percepire, ragionare, apprendere e agire in maniera efficace nel suo ambiente.

2.4.3.1 Modular Memory Components
	Un aspetto distintivo delle architetture CoALA è la loro enfasi sulla gestione modulare e sofisticata della memoria, un elemento cruciale che amplia significativamente le capacità innate dei soli LLMs (Wang et al., 2024). Mentre gli LLM possiedono una vasta “memoria parametrica” implicita, acquisita durante il pre-addestramento (i loro pesi), questa manca di un accesso diretto, di una struttura organizzata per il recall specifico e di una capacità di aggiornamento incrementale e selettivo post-addestramento (Brown et al., 2020). Le architetture CoALA integrano quindi diversi tipi di memoria esterna, ciascuna con una funzione specifica, gestite del ‘cervello’ LLM dell’agente (Wang et al., 2024).
Queste componenti si dividono tipicamente in tre tipologie, di cui la prima è una memoria procedurale. Questa componente si riferisce alle competenze operative e agli schemi di azione dell’agente. In un contesto CoALA, la memoria procedurale si manifesta in due forme interconnesse (Wang et al., 2024). Da un lato, include i parametri stessi dell’LLM, che rappresentano una vasta conoscenza procedurale implicita, inclusi patterns linguistici e logiche apprese durante il pre-training. Questi parametri, sebbene difficili da interpretare direttamente, offrono una notevole flessibilità e capacità di scalatura. Dall’altro lato, la memoria procedurale comprende il codice esplicito dell’agente, ovvero algoritmi, script o regole deterministiche scritte dal programmatore che definiscono comportamenti specifici o l’uso di tools esterni. Questi elementi sono interpretabili e garantiscono una maggiore controllabilità, ma possono risultare fragili se l’ambiente o il task deviano dalle assunzioni iniziali (Wang et al., 2024). L’integrazione di entrambi permette all’agente di bilanciare la flessibilità generativa dell’LLM con la precisione di esecuzione di procedure definite.
Una successiva componente è la memoria semantica. Essa è dedicata alla conservazione di conoscenze fattuali e relazioni tra concetti relative all’ambiente e al mondo in generale. Spesso implementata tramite knowledge bases , triple store  (es. RDF), o knowledge graphs  (es. Neo4J), la memoria semantica fornisce all’agente un accesso strutturato e interrogabile a informazioni verificabili che vanno oltre quelle apprese implicitamente dall’LLM (Wang et al., 2024). Questo permette all’agente di groundare le proprie risposte nella realtà, mitigando le ‘allucinazioni’ e incrementando l’accuratezza in compiti che richiedono conoscenza specifica e aggiornata. Esempi includono database di fatti, ontologie di dominio o embeddings vettoriali che rappresentano concetti e la loro relazione semantica. I meccanismi di Retrieval Augmented Generation (RAG) sono un esempio dell'uso dinamico della memoria semantica, dove l'LLM interroga una base di conoscenza esterna per recuperare informazioni pertinenti prima di formulare una risposta.
Infine, è presente una componente di memoria episodica: Similmente alla memoria episodica umana, questa componente registra le esperienze passate specifiche dell’agente, comprese le interazioni con l’ambiente, le sequenze di azioni intraprese, i risultati ottenuti e gli stati interni dell’agente. Questa memoria può essere implementata come un log cronologico degli eventi, un database di interazioni o un replay buffer (nel contesto del reinforcement learning) (Wang et al., 2024). La memoria episodica è cruciale per l’apprendimento dall’esperienza, poiché consente all’agente di ‘riflettere’ sui propri errori o successi, di adattare le proprie strategie e di migliorare le capacità di pianificazione a lungo termine. Un LA potrebbe archiviare le conversazioni precedenti o le osservazioni del mondo per migliorare le future interazioni.
La gestione di queste diverse forme di memoria è compito dell'LLM, che funge da "controller" per l'accesso, l'aggiornamento e l'integrazione delle informazioni provenienti da ciascun modulo, consentendo all'agente di costruire una comprensione olistica e dinamica del proprio ambiente e delle proprie operazioni (Wang et al., 2024).

2.4.3.2 Structured Action Space
	All’interno delle architetture CoALA, oltre alla gestione modulare della memoria, un ruolo centrale è rivestito dallo Structured Action Space, inteso come la capacità dell’agente di compiere azioni concrete e deliberatamente strutturate nel proprio ambiente. Tale concetto segna una netta differenza rispetto agli LLM stand-alone, i quali risultano confinati alla produzione di sequenze di token in risposta a un prompt. Un LA, al contrario, necessita di un meccanismo che consenta di tradurre decisioni cognitive espresse in linguaggio naturale in operazioni eseguibili, capaci di incidere sull’ambiente esterno o di intervenire sulla gestione delle proprie risorse interne. In questa prospettiva, l’Action Space assume la funzione di un repertorio definito di strumenti e funzioni che l’agente può selezionare e attivare per governare l’interazione con il mondo circostante.
	La natura strutturata di questo spazio consente di equipaggiare l’agente con un insieme di tools o interfacce finalizzate all’interazione con risorse digitali. L’impiego di motori di ricerca per accedere a informazioni aggiornate, di calcolatrici per la risoluzione di problemi numerici o di API per l’interrogazione di basi di dati rappresenta un’estensione significativa delle capacità di un LLM. In questo contesto, il modello linguistico agisce di fatto come controller cognitivo, determinando quale strumento attivare, in quale momento e con quali parametri, sulla base delle percezioni disponibili e del proprio processo interno di ragionamento (Yao et al.,2022b). Tale dinamica permette di oltrepassare i limiti intrinseci alla conoscenza statica di un modello pre-addestrato, colmando lacune informative e ampliando la gamma di azioni eseguibili.
	Lo Structured Action Space, tuttavia, non si riduce alla semplice invocazione di strumenti astratti, ma include la possibilità di interagire con l’ambiente in modo diretto e contestuale. Ciò si traduce, ad esempio, nella capacità di inviare comandi a sistemi fisici, come avviene nella robotica, di modificare lo stato di un ambiente simulato o di gestire lo scambio con servizi e piattaforme digitali. In questi casi, l’LLM è responsabile della generazione della logica sottostante, mentre l’effettiva esecuzione delle azioni viene affidata a moduli specializzati che traducono l’intenzione linguistica in comandi operativi. Ne risulta un ciclo di percezione e azione chiuso e adattivo, capace di integrare il feedback derivante dall’ambiente per ricalibrare dinamicamente il comportamento dell’agente (Wang et al., 2024).

2.4.3.3 Generalized Decision-Making Process
	Il terzo pilastro fondamentale all’interno dell’architettura CoALA è il Generalized Decision-Making Process, il quale conferisce ai LA la capacità di andare oltre l’esecuzione di sequenze predefinite o l’attivazione isolata di tools. Esso costituisce il nucleo di una sorta di ‘intelligenza strategica’, poiché permette di ragionare, pianificare e riflettere criticamente sulle proprie azioni e sullo stato interno al fine di prendere decisioni complesse orientate al raggiungimento di obiettivi specifici (Wang et al., 2024).
Una componente essenziale di tale processo è costituita dalle capacità di planning e reasoning. Grazie alla loro conoscenza enciclopedica e alla flessibilità nella manipolazione del linguaggio, gli LLMs sono impiegati come meccanismi di pensiero in grado di decomporre obiettivi complessi in sotto-obiettivi gestibili, formulare strategie di risoluzione e anticipare le conseguenze delle scelte effettuate. La pianificazione non avviene secondo i canoni della panificazione simbolica tradizionale, fondata su operatori formali, bensì attraverso una forma emergente di deduzione linguistica e conversazionale. Framework come il Tree-of-Thought sfruttano tale proprietà per esplorare percorsi multipli di ragionamento, stimarne la probabilità di successo e selezionare la traiettoria più promettente, garantendo un approccio adattivo e dinamico al problem solving (Yao et al., 2023).
	Un secondo aspetto riguarda la riflessione e l’auto-correzione. L’agente cognitivo non si limita a eseguire piani, ma valuta criticamente gli esiti delle proprie azioni rispetto agli obiettivi perseguiti, identificando inefficienze ed errori e generando strategie di miglioramento. Questa riflessione, supportata dall’LLM che può analizzare log di esecuzione ed estrarne spunti correttivi, consente un apprendimento continuo e incrementale, rafforzando l’adattabilità e la capacità di ottimizzare nel tempo i propri comportamenti.
	Il Generalized Decision-Making Process si intreccia inoltre con la gestione delle memorie, poiché ogni percezione acquisita, ogni decisione presa e ogni risultato ottenuto contribuiscono ad aggiornare dinamicamente i diversi moduli della memoria interna, che includono componenti procedurali, semantiche ed episodiche (Wang et al., 2024). Questo continuo feedback loop garantisce che lo stato interno dell’agente rifletta in tempo reale le conoscenze acquisite, preservandole e rendendole disponibili per processi decisionali futuri.
	In questo modo, il processo decisionale generalizzato consente all’agente di manifestare un comportamento proattivo e adattivo, fondato su un ciclo costante di osservazione, ragionamento, pianificazione, azione e riflessione, che lo distingue da modelli puramente reattivi e lo avvicina a forme di intelligenza autonoma più mature.

2.4.4 Vantaggi delle architetture cognitive
	Le architetture cognitive come CoALA rappresentano un approccio che non si limita a compensare i limiti degli LLM, ma ne valorizza le potenzialità collocandole all’interno di una struttura più ‘organica’ e funzionale. L’integrazione di memorie modulari e di un processo decisionale generalizzato consente infatti di costruire sistemi che non dipendono esclusivamente dalla generazione linguistica, ma che sono in grado di attingere a conoscenze persistenti, riflettere sulle proprie azioni e mantenere una continuità nel tempo. Questo produce agenti più robusti, meno soggetti a fenomeni di incoerenza o a errori dovuti alla parzialità e all’obsolescenza dei dati interni, e più capaci di affrontare situazioni impreviste con strategie adattive.
	Un aspetto particolarmente rilevante è l’uso più efficiente delle risorse: l’LLM viene impiegato come motore di ragionamento ad alto livello, mentre l’esecuzione concreta delle azioni viene affidata a strumenti specializzati o a componenti mirati. In questo modo, l’agente non solo riduce i costi computazionali, ma può anche integrare in maniera sinergica moduli diversi, sfruttando punti di forza complementari. Allo stesso tempo, la divisione in componenti favorisce una maggiore trasparenza e tracciabilità dell’intero processo di decisione (Wang et al., 2024). Infatti, analizzando le memorie semantiche ed episodiche, è possibile ricostruire i motivi alla base delle scelte compiute, un aspetto che contribuisce non solo alla validazione tecnica, ma anche alla costruzione verso sistemi sempre più autonomi.
	Questa caratteristica rende CoALA non soltanto un’architettura promettente dal punto di vista della ricerca, ma anche un vero e proprio paradigma per la realizzazione di agenti destinati a operare in scenari complessi e dinamici. La capacità di apprendere dalle esperienze, di aggiornare in maniera continua lo stato interno e di mantenere una coerenza di lungo termine permette di immaginare sistemi che non si limitano a reagire a stimoli, ma che sviluppano forme di comportamento più durature, proattive e sofisticate.

2.5 Sviluppo e orchestrazione di AI Agents
	Lo sviluppo e l'orchestrazione degli AI Agents basati su Large Language Models costituiscono una delle frontiere più dinamiche e promettenti nell'ambito dell'intelligenza artificiale contemporanea. Mentre architetture cognitive come CoALA hanno fornito una cornice concettuale per integrare memoria, ragionamento e processi decisionali, la traduzione di tali concetti in sistemi operativi concreti richiede metodologie e strumenti avanzati. Questi ultimi sono essenziali per gestire la complessità derivante dalla necessità di coordinare moduli eterogenei e abilitare comportamenti agentici sofisticati. La costruzione di un AI Agent, infatti, trascende il mero impiego di un LLM, implicando la definizione di come esso percepisce l'ambiente (perception), di quali strumenti (tools) utilizza per interagire (action), di come organizza le proprie memorie (memory) e di quali meccanismi adotta per apprendere e adattarsi (learning).
	L’orchestrazione emerge come il nucleo operativo fondamentale in questo contesto, responsabile della gestione del flusso di dati, del coordinamento delle decisioni e dell'integrazione delle diverse componenti dell'agente. Essa garantisce la coerenza nei cicli percezione-azione-riflessione che sottendono l'autonomia e l'efficacia dell'agente.
Un’orchestrazione sofisticata permette all'agente di eseguire prompt engineering dinamico, scegliere strumenti appropriati, aggiornare la propria base di conoscenza e persino intraprendere self-correction o self-improvement in risposta a feedback ambientali o interni.
Questa visione trova riscontro in applicazioni settoriali concrete dove gli AI Agents stanno trasformando i paradigmi operativi. Nel customer service e nel supporto tecnico, gli agenti intelligenti non si limitano a risposte predefinite, ma recuperano informazioni da basi di conoscenza aziendali, personalizzano le interazioni e ottimizzano l'efficienza operativa umana. In ambito finanziario, agenti specializzati analizzano flussi di dati eterogenei, dalle notizie economiche ai report di mercato, per supportare decisioni d'investimento data-driven e ampliare le capacità analitiche dei professionisti. Nel settore dello sviluppo software, si assiste alla transizione da strumenti di code assistance ad agenti capaci di generare architetture complete, identificare vulnerabilità o accelerare i processi di progettazione. Questi esempi sottolineano una tendenza emergente: gli agenti non sono più concepiti come semplici ausili, ma come collaboratori autonomi in grado di assumere responsabilità operative e strategiche, richiedendo piattaforme robuste per la loro gestione e interazione (Park et al., 2023).

2.5.1 Framework per la Costruzione di AI Agents
	L’esigenza di facilitare lo sviluppo, la gestione e l’orchestrazione di AI Agents ha portato alla nascita di una nuova generazione di frameworks progettati specificamente per questo scopo. Questi framework mirano a fornire astrazioni e strumenti che permettono agli sviluppatori di concentrarsi sulla logica dell’agente piuttosto che sulle complessità sottostanti dell’integrazione di LLMS, strumenti esterni e moduli di memoria. Essi giocano un ruolo cruciale nel trasformare le capacità degli LLMs in intelligenza agentica funzionale e adattabile. Questi frameworks sono stati ideati per affrontare diverse sfide, tra cui la gestione del context windows degli LLMs, la capacità di interagire con il mondo esterno tramite tools, la gestione della memoria a breve e lungo termine, e l’implementazione di meccanismi di ragionamento complessi come il chain-of-thought o il tree-of-thought.
	La loro architettura tipicamente supporta la modularità, permettendo la combinazione di diversi componenti per creare agenti con comportamenti complessi e specifici per il dominio di applicazione. Questo si traduce nella possibilità di estendere le capacità degli LLMs oltre la generazione di testo, consentendo loro di ottenere le abilità di cui si è parlato in precedenza. La scelta di un framework piuttosto che un altro dipende dal livello di complessità richiesto, dal dominio applicativo specifico, dalle esigenze di scalabilità e dalle preferenze progettuali dello sviluppatore o del team di sviluppo. Tuttavia, un obiettivo condiviso da tutti questi strumenti è proprio l’intento di abbassare la barriera d’ingresso per la costruzione di AI Agents robusti e intelligenti, fornendo un ecosistema con cui orchestrare l’intelligenza distribuita e trasformare idee teoriche in soluzione concrete ed operative.

2.5.1.1 LangChain e LangGraph: Architetture Modulari e Grafo-Based
	Nell’attuale panorama dei framework per lo sviluppo di AI Agents basati su LLMs, LangChain si è affermato come una delle soluzioni più influenti e diffusamente adottate. La sua architettura si caratterizza per essere modulare, progettata quindi per facilitare l’integrazione e l’orchestrazione di diverse componenti necessarie per la costruzione di agenti intelligenti.
	LangChain si fonda sul concetto di “catene” (chains), attraverso le quali è possibile sequenziare chiamate a LLMs, interazioni con fonti dati esterne (mediante RAG) e l’impiego di tools specifici.
	All’interno di LangChain, i models rappresentano astrazioni per l’interazione con differenti tipologie di modelli linguistici, mentre i prompts costituiscono strumenti per la gestione e l’ottimizzazione dei comandi testuali, includendo template e parser  di output. Gli indexes consentono l’integrazione con fonti dati esterne, funzione cruciale per le applicazioni RAG, mentre le chains definiscono sequenze di componenti che realizzano flussi di lavoro predefiniti, combinando diversi passaggi logici. Gli agents costituiscono il nucleo operativo del framework: in essi un modello linguistico impiega i tools disponibili e una logica di ragionamento per decidere la sequenza di azioni più opportuna in base a un determinato input e a un obiettivo da raggiungere. Infine, la memory garantisce la persistenza dello stato e la gestione della storia delle interazioni risultando essenziale per le conversazioni e i compiti di lunga durata.
	L’evoluzione naturale di LangChain ha condotto allo sviluppo di LangGraph, un’estensione progettata per modellare e gestire comportamenti agentici complessi, superando la linearità delle tradizionali chains. LangGraph introduce un paradigma di programmazione basato su stati e grafi diretti aciclici (DAGs) o ciclici, in modo da supportare meccanismi di feedback e processi di self-correction (LangChain, s.d.). Questo approccio consente la realizzazione di flussi di lavoro non lineari e ciclici, indispensabili per agenti capaci di ragionamento iterativo, di gestione di stati complessi e di cooperazione multi-agente. Gli agenti, infatti, possono riflettere sui propri output, correggere errori o ripianificare le azioni sulla base di nuove informazioni o risultati intermedi; possono inoltre mantenere e aggiornare uno stato persistente tra i vari passaggi del grafo, sviluppando una memoria operativa più sofisticata; infine, il design basato sui grafi si presta naturalmente alla costruzione di sistemi multi-agente, nei quali più entità collaborano o competono per il raggiungimento di obiettivi comuni, coordinando il flusso decisionale e la comunicazione reciproca.
	LangGraph, quindi, supera i limiti delle chains lineari di LangChain, fornendo una maggiore flessibilità per implementare logiche di controllo più avanzate e comportamenti adattivi. La sua architettura permette di definire nodi (corrispondenti a LLMs, tools, o logiche personalizzate) e bordi (che rappresentano le transizioni di stato), rendendo esplicita la logica decisionale e il flusso di esecuzione dell'agente. Questo facilita la progettazione di agenti reattivi, proattivi e collaborativi, capaci di operare in ambienti complessi con un alto grado di autonomia e resilienza.

2.5.1.2 Modelli Collaborativi e Sistemi Multi-Agente: AutoGen e CrewAI
	L'evoluzione degli AI Agents ha rapidamente evidenziato che la complessità dei task del mondo reale spesso supera le capacità di un singolo agente. Da questa constatazione è emersa l'importanza dei sistemi multi-agente, nei quali entità autonome non solo operano individualmente, ma collaborano e interagiscono per raggiungere obiettivi comuni che sarebbero altrimenti irraggiungibili (Wooldridge, 2009). In tale scenario, il concetto di orchestration si espande, inglobando la gestione delle interazioni, della comunicazione e del coordinamento tra agenti eterogenei. Questa necessità ha stimolato lo sviluppo di framework specifici, come AutoGen e CrewAI, concepiti per semplificare la costruzione di ambienti collaborativi per AI Agents, fornendo astrazioni che facilitano la gestione delle dinamiche inter-agente.
	AutoGen, sviluppato da Microsoft Research, rappresenta una soluzione versatile per la costruzione di applicazioni multi-agente che sfruttano la potenza degli LLMs. La sua architettura promuove la creazione di agenti con ruoli e competenze distinte, capaci di comunicare efficacemente per il raggiungimento di un obiettivo condiviso. Questo framework si distingue per la sua flessibilità nella gestione di dialoghi inter-agente: consente la definizione di varie tipologie di agenti, come ad esempio un assistant agent o un user proxy agent, che possono scambiarsi messaggi in un formato conversazionale, simulando in modo convincente le interazioni umane. Tale capacità di comunicazione è intrinsecamente legata all'automazione del workflow, dove agenti diversi possono succedersi o operare in parallelo, secondo una logica interna e in base agli output ottenuti.
Inoltre, AutoGen permette di dotare gli agenti di tools esterni quali funzioni, API o script Python, estendendo le loro possibilità di interazione con il mondo esterno, di accesso a informazioni e di esecuzione di calcoli. Un aspetto di particolare rilevanza è la sua capacità di supportare sia interazioni completamente autonome tra agenti sia l'intervento umano nel ciclo di dialogo, offrendo all'utente la possibilità di fornire feedback o direttive. 
Questo approccio modulare e conversazionale abilita scenari in cui un Assistant Agent può coordinarsi con un User Proxy Agent per chiarire un problema, per poi delegare la risoluzione a un Developer Agent specializzato, emulando la divisione dei compiti in un team umano e migliorando l'efficienza e la qualità dei risultati.
Parallelamente, CrewAI emerge come un framework innovativo che focalizza la propria attenzione sulla costruzione di "equipaggi" (crews) di agenti AI, ciascuno con ruoli ben definiti, obiettivi chiari (goals) e tools specifici (CrewAI, s.d.). La sua peculiarità risiede nell'orchestrare una collaborazione altamente strutturata, mimando le dinamiche di squadra per l'esecuzione di progetti o task complessi. 
In CrewAI, a ogni agente viene assegnato un ruolo distinto, come un Researcher, un Writer o un Editor, conferendogli competenze e responsabilità specifiche all'interno del team. Agli agenti sono poi assegnati task dettagliati e obiettivi precisi, con la possibilità di definire dipendenze tra i task stessi e strategie per la loro gestione. Questo framework promuove attivamente processi di collaborazione avanzati, quali l'iterazione e il peer review tra agenti, al fine di affinare gli output e assicurare la qualità del lavoro finale. 
L'orchestration fornita da CrewAI si concentra sulla delegation dei task e sulla gestione del flusso di lavoro tra i membri dell'equipaggio, facilitando una pianificazione e esecuzione coesa. Questo lo rende particolarmente efficace per task che richiedono un approccio multidisciplinare, come la generazione di contenuti complessi, l'analisi approfondita di dati o la ricerca e sintesi di informazioni. L'idea fondamentale è che, grazie a ruoli definiti e obiettivi comuni in un contesto di squadra, le sinergie emergenti dall'interazione tra gli agenti possano condurre a risultati qualitativamente superiori rispetto all'operato di agenti individuali.
In sintesi, sia AutoGen che CrewAI rappresentano un avanzamento significativo verso la realizzazione di AI Agents collaborativi, capaci di risolvere problematiche articolate attraverso meccanismi sofisticati di comunicazione, delega e coordinamento, avvicinandosi sempre più alle capacità di problem-solving dei team umani.

2.5.2 Altri Framework e Strumenti Emergenti
Oltre ai framework precedentemente analizzati, LangChain/LangGraph, AutoGen e CrewAI, il panorama dell'orchestrazione degli AI Agents è in continua evoluzione, con l'emergere di ulteriori strumenti e piattaforme che contribuiscono a democratizzare e specializzare lo sviluppo. Questi framework alternativi spesso si concentrano su nicchie specifiche o propongono approcci innovativi all'integrazione di LLMs, gestione della memoria, pianificazione e tool use.
Tra questi, si possono annoverare soluzioni come MetaGPT, che si focalizza sulla generazione di software basato sulla collaborazione multi-agente, permettendo agli agenti di ricoprire ruoli come architetti, project manager, ingegneri e tester per produrre intere sequenze di code e documentazione. Questo framework enfatizza la standardizzazione del flusso di lavoro tra agenti, emulando un processo di sviluppo software gerarchico e professionale.
Un'altra categoria include piattaforme che offrono ambienti di sviluppo visivi o low-code per agenti, facilitando la prototipazione rapida e la configurazione di workflows complessi senza una profonda conoscenza di programmazione. Alcuni framework come AgentVerse o MindAgent propongono ambienti di simulazione multi-agente per studiare comportamenti complessi ed emergent intelligence, spingendo i confini della ricerca sugli agenti collaborativi in contesti dinamici e sociali.
L'eterogeneità di queste soluzioni riflette la rapida espansione del settore e la diversità delle esigenze applicative. La scelta del framework più adatto dipenderà dalle specifiche del progetto, dalla complessità desiderata per l'agente o il sistema multi-agente, dal dominio applicativo e dal livello di controllo e personalizzazione richiesto dallo sviluppatore. In ogni caso, l'obiettivo comune di questi strumenti è fornire un ecosistema robusto e accessibile per la creazione di AI Agents sempre più autonomi e sofisticati.





















References
Bommasani, R., Hudson, R., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On the Opportunities and Risks of Foundation Models. arXiv preprint arXiv:2108.07258.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems (NeurIPS), 33.
Cameron R. Jones, Benjamin K. Bergen, Large Language Models Pass the Turing Test, arXiv:2503.23674 [cs.CL], marzo 2025
Campbell, M., Hoane Jr, A. J., & Hsu, F. H. (2002). Deep Blue. Artificial Intelligence, 134(1-2), 57-83.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations (ICLR).
Hao, N., et al. (2023). Reasoning and Planning with Large Language Models: A Survey. arXiv preprint arXiv:2310.02167.
CrewAI. (s.d.). Official Website and Documentation.
Wu, W., Fang, Y., Cao, H., Xiong, S., & Li, M. (2023). AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. arXiv preprint arXiv:2308.08155.
McCarthy, J., et al. (1955). A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
Radford, A., Kim, J. W., Xu, T., Brockman, G., McAlear, C., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML) (pp. 8748-8763). PMLR.
Russell, S. J., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd ed.). Prentice Hall. (Capitolo 1, Introduzione).
Park, J. S., Joshi, J., Newman, W., Mir, A., Musser, M., Palomino, A., ... & Cai, M. (2023). Generative Agents: Interactive Simulacra of Human Behavior. arXiv preprint arXiv:2304.03442.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems (NeurIPS), 30.
Wang, G., Xie, R., Jiang, T., & Che, C. (2024). Cognitive Architectures for Language Agents. Transactions on Machine Learning Research.
Wooldridge, M. (2009). An introduction to multiagent systems (2nd ed.). John Wiley & Sons.
Yao, S., & Narasimhan, K. (2023). Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.
Yao, S., et al. (2022b). ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629.
Wooldridge, M. J. (2009). An Introduction to MultiAgent Systems. John Wiley & Sons.
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Roberts, A., ... & Le, Q. V. (2022). Emergent Abilities of Large Language Models. Transactions on Machine Learning Research.
Chase, H. (2022). LangChain. GitHub repository. https://github.com/langchain-ai/langchain
LangChain. (s.d.). LangGraph Documentation. https://langchain.com/docs/langgraph


